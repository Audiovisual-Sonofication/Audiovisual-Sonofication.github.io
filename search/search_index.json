{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AudioVisual Application for Interactive Medical Data Exploration Documentation For the Projects Code visit: [https://github.com/Audiovisual-Sonofication/Plugin.git] Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Home"},{"location":"#welcome-to-audiovisual-application-for-interactive-medical-data-exploration-documentation","text":"For the Projects Code visit: [https://github.com/Audiovisual-Sonofication/Plugin.git]","title":"Welcome to AudioVisual Application for Interactive Medical Data Exploration Documentation"},{"location":"#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"about/","text":"About the Project Project Overview AudioVisual Application for Interactive Medical Data Exploration is an innovative tool designed to enhance the interpretation of medical imaging through audiovisual sonification. Developed as an Imfusion Plugin in C++, this tool integrates advanced computational techniques with artistic expression to transform medical image data into audio and physical models. By sending OSC messages to Processing, the plugin facilitates the creation of sound-emitting physical models, enabling a multisensory approach to medical diagnostics and research. The project aims to provide clinicians and researchers with a novel method to perceive and interact with medical data, thereby enhancing understanding and potentially uncovering new insights into complex medical images. Development Team Heidi Albarazi Role: Developer Background: Bachelor student in Computer Science Institution: Technical University of Munich Bio: Heidi Albarazi is currently completing her Bachelor's degree in Computer Science with an application subject in medicine at the Technical University of Munich. With a keen interest in medical imaging and its applications, Heidi has focused on integrating audiovisual elements into medical diagnostics tools to enhance user interaction and data interpretation. Demil Omerovic Role: Developer Background: Bachelor student in Computer Science Institution: Technical University of Munich Bio: Demil Abramovic is a Bachelor student at the Technical University of Munich, specializing in computer science with an application subject in medicine. His work on this project involves leveraging his programming expertise to bridge the gap between traditional medical imaging techniques and innovative sonification processes. Institutional Affiliation Chair for Computer Aided Medical Procedures & Augmented Reality Location: Technical University of Munich Website: CAMP Lead: Prof. Dr. Nassir Navab Project Supervisor: Sasan Matinfar The Chair for Computer Aided Medical Procedures & Augmented Reality at TUM is renowned for its cutting-edge research and development in the field of computer-assisted interventions. Under the leadership of Prof. Dr. Nassir Navab and the guidance of Sasan Matinfar, the chair provides a robust platform for innovation in medical technologies, focusing on enhancing clinical workflows through augmented reality and advanced computing. Project Goals The primary objectives of this project include: Innovative Interpretation: Utilize sonification to provide a new perspective on medical image data, aiding in more intuitive understanding and analysis. Enhanced Interaction: Offer clinicians and researchers an interactive tool that augments the visual data analysis with auditory cues. Research Contribution: Contribute to the field of medical imaging by integrating audiovisual sonification, setting the groundwork for future research and application. This project represents a collaborative effort to push the boundaries of how medical imaging is traditionally experienced, aiming to enrich the diagnostic process through an integrative audiovisual approach.","title":"About"},{"location":"about/#about-the-project","text":"","title":"About the Project"},{"location":"about/#project-overview","text":"AudioVisual Application for Interactive Medical Data Exploration is an innovative tool designed to enhance the interpretation of medical imaging through audiovisual sonification. Developed as an Imfusion Plugin in C++, this tool integrates advanced computational techniques with artistic expression to transform medical image data into audio and physical models. By sending OSC messages to Processing, the plugin facilitates the creation of sound-emitting physical models, enabling a multisensory approach to medical diagnostics and research. The project aims to provide clinicians and researchers with a novel method to perceive and interact with medical data, thereby enhancing understanding and potentially uncovering new insights into complex medical images.","title":"Project Overview"},{"location":"about/#development-team","text":"","title":"Development Team"},{"location":"about/#heidi-albarazi","text":"Role: Developer Background: Bachelor student in Computer Science Institution: Technical University of Munich Bio: Heidi Albarazi is currently completing her Bachelor's degree in Computer Science with an application subject in medicine at the Technical University of Munich. With a keen interest in medical imaging and its applications, Heidi has focused on integrating audiovisual elements into medical diagnostics tools to enhance user interaction and data interpretation.","title":"Heidi Albarazi"},{"location":"about/#demil-omerovic","text":"Role: Developer Background: Bachelor student in Computer Science Institution: Technical University of Munich Bio: Demil Abramovic is a Bachelor student at the Technical University of Munich, specializing in computer science with an application subject in medicine. His work on this project involves leveraging his programming expertise to bridge the gap between traditional medical imaging techniques and innovative sonification processes.","title":"Demil Omerovic"},{"location":"about/#institutional-affiliation","text":"Chair for Computer Aided Medical Procedures & Augmented Reality Location: Technical University of Munich Website: CAMP Lead: Prof. Dr. Nassir Navab Project Supervisor: Sasan Matinfar The Chair for Computer Aided Medical Procedures & Augmented Reality at TUM is renowned for its cutting-edge research and development in the field of computer-assisted interventions. Under the leadership of Prof. Dr. Nassir Navab and the guidance of Sasan Matinfar, the chair provides a robust platform for innovation in medical technologies, focusing on enhancing clinical workflows through augmented reality and advanced computing.","title":"Institutional Affiliation"},{"location":"about/#project-goals","text":"The primary objectives of this project include: Innovative Interpretation: Utilize sonification to provide a new perspective on medical image data, aiding in more intuitive understanding and analysis. Enhanced Interaction: Offer clinicians and researchers an interactive tool that augments the visual data analysis with auditory cues. Research Contribution: Contribute to the field of medical imaging by integrating audiovisual sonification, setting the groundwork for future research and application. This project represents a collaborative effort to push the boundaries of how medical imaging is traditionally experienced, aiming to enrich the diagnostic process through an integrative audiovisual approach.","title":"Project Goals"},{"location":"code/","text":"Code Documentation System Requirements Processing IDE ImFusion cmake qtCreator miPhysics library for physics simulation PeasyCam library for 3D camera control oscP5 library for handling OSC messages netP5 library for networking capabilities Phymodel For this please refer to the miPhysics Library documentation which you can find in Processing. ShapeGenerator Overview The ShapeGenerator is an abstract class designed to facilitate shape generation within a physics engine. This class extends PhyModel and includes methods for setting up and manipulating physical properties, as well as adding nodes and connections based on geometric parameters. Properties m_mass : Default mass of nodes (default: 1.0). m_stiffness : Spring stiffness (default: 0.01). m_damping : Damping coefficient (default: 0.001). m_size : Size of the nodes (default: 1.0). m_dist : Default distance between nodes (default: 1.0). m_l0 : Natural length of springs (default: 1.0). m_neighbors : Number of neighboring nodes each node should connect to (default: 2). plane2D : Determines if the shapes are generated in a 2D plane (boolean). m_generated : Indicates if the shape has been generated (boolean, initially false ). Constructor public ShapeGenerator(String name, Medium m) Parameters: name: Name of the shape generator. m: Medium in which the shapes will be generated. Methods init() Initializes the shape generation. If shapes have not been generated (m_generated is false), the program will terminate with an error message. generate() Abstract method that must be implemented by subclasses to define specific shape generation logic. setParams(double mass, double stiffness, double damping) Sets the mass, stiffness, and damping parameters. Parameters: mass: Mass of the nodes. stiffness: Stiffness of the springs. damping: Damping coefficient. setGeometry(double dist, double l0) Sets the geometric properties of the shape, including the distance between nodes and the natural length of the springs. Parameters: dist: Distance between nodes. l0: Natural length of the springs. addNodeAt(int x, int y, int z) Adds a node at specified coordinates. Parameters: x: X-coordinate of the node. y: Y-coordinate of the node. z: Z-coordinate of the node. addConnectionFor(int x1, int y1, int z1, int x2, int y2, int z2) Creates a connection between two nodes defined by their coordinates. Parameters: x1, y1, z1: Coordinates of the first node. x2, y2, z2: Coordinates of the second node. processPoints(int numClosest) Processes all nodes to find and connect each node to its closest neighbors based on a specified number of closest points. Parameters: numClosest: Number of closest points to connect to each node. findClosestPoints(Mass currentPoint, ArrayList of Mass points, int numClosest) Finds and returns a list of the closest points to a given point, based on the specified number of closest neighbors. Parameters: currentPoint: The point from which distances are measured. points: List of all points. numClosest: Number of closest points to find. euclideanDistance(Mass p1, Mass p2) Calculates and returns the Euclidean distance between two points. Parameters: p1: First point. p2: Second point. Private Helper Methods getNodeName(int x, int y, int z) Generates a standardized node name based on its coordinates. Parameters: x: X-coordinate of the node. y: Y-coordinate of the node. z: Z-coordinate of the node. Returns: A string representing the node name, formatted as m_x_y_z. Example Usage CircularShapeGenerator myShapeGen = new CircularShapeGenerator(\"MyShape\", someMedium); myShapeGen.setParams(2.0, 0.05, 0.002); myShapeGen.setGeometry(1.5, 1.0); myShapeGen.generate();","title":"Code"},{"location":"code/#code-documentation","text":"","title":"Code Documentation"},{"location":"code/#system-requirements","text":"Processing IDE ImFusion cmake qtCreator miPhysics library for physics simulation PeasyCam library for 3D camera control oscP5 library for handling OSC messages netP5 library for networking capabilities","title":"System Requirements"},{"location":"code/#phymodel","text":"For this please refer to the miPhysics Library documentation which you can find in Processing.","title":"Phymodel"},{"location":"code/#shapegenerator","text":"","title":"ShapeGenerator"},{"location":"code/#overview","text":"The ShapeGenerator is an abstract class designed to facilitate shape generation within a physics engine. This class extends PhyModel and includes methods for setting up and manipulating physical properties, as well as adding nodes and connections based on geometric parameters.","title":"Overview"},{"location":"code/#properties","text":"m_mass : Default mass of nodes (default: 1.0). m_stiffness : Spring stiffness (default: 0.01). m_damping : Damping coefficient (default: 0.001). m_size : Size of the nodes (default: 1.0). m_dist : Default distance between nodes (default: 1.0). m_l0 : Natural length of springs (default: 1.0). m_neighbors : Number of neighboring nodes each node should connect to (default: 2). plane2D : Determines if the shapes are generated in a 2D plane (boolean). m_generated : Indicates if the shape has been generated (boolean, initially false ).","title":"Properties"},{"location":"code/#constructor","text":"public ShapeGenerator(String name, Medium m) Parameters: name: Name of the shape generator. m: Medium in which the shapes will be generated.","title":"Constructor"},{"location":"code/#methods","text":"init() Initializes the shape generation. If shapes have not been generated (m_generated is false), the program will terminate with an error message. generate() Abstract method that must be implemented by subclasses to define specific shape generation logic. setParams(double mass, double stiffness, double damping) Sets the mass, stiffness, and damping parameters. Parameters: mass: Mass of the nodes. stiffness: Stiffness of the springs. damping: Damping coefficient. setGeometry(double dist, double l0) Sets the geometric properties of the shape, including the distance between nodes and the natural length of the springs. Parameters: dist: Distance between nodes. l0: Natural length of the springs. addNodeAt(int x, int y, int z) Adds a node at specified coordinates. Parameters: x: X-coordinate of the node. y: Y-coordinate of the node. z: Z-coordinate of the node. addConnectionFor(int x1, int y1, int z1, int x2, int y2, int z2) Creates a connection between two nodes defined by their coordinates. Parameters: x1, y1, z1: Coordinates of the first node. x2, y2, z2: Coordinates of the second node. processPoints(int numClosest) Processes all nodes to find and connect each node to its closest neighbors based on a specified number of closest points. Parameters: numClosest: Number of closest points to connect to each node. findClosestPoints(Mass currentPoint, ArrayList of Mass points, int numClosest) Finds and returns a list of the closest points to a given point, based on the specified number of closest neighbors. Parameters: currentPoint: The point from which distances are measured. points: List of all points. numClosest: Number of closest points to find. euclideanDistance(Mass p1, Mass p2) Calculates and returns the Euclidean distance between two points. Parameters: p1: First point. p2: Second point.","title":"Methods"},{"location":"code/#private-helper-methods","text":"getNodeName(int x, int y, int z) Generates a standardized node name based on its coordinates. Parameters: x: X-coordinate of the node. y: Y-coordinate of the node. z: Z-coordinate of the node. Returns: A string representing the node name, formatted as m_x_y_z.","title":"Private Helper Methods"},{"location":"code/#example-usage","text":"CircularShapeGenerator myShapeGen = new CircularShapeGenerator(\"MyShape\", someMedium); myShapeGen.setParams(2.0, 0.05, 0.002); myShapeGen.setGeometry(1.5, 1.0); myShapeGen.generate();","title":"Example Usage"},{"location":"related-work/","text":"Related Work Sonification for Process Monitoring in Highly Sensitive Surgical Tasks This publication discusses innovative sonification methods for monitoring fluid-related processes in sensitive surgical environments. The techniques convert data on fluid rates and volumes into auditory signals, aiming to enhance situational awareness. The effectiveness of these methods was evaluated through user studies to determine the best auditory representations for surgical use. Citation: Matinfar, S., Hermann, T., Seibold, M., F\u00fcrnstahl, P., Farshad, M., & Navab, N. (In Press). Sonification for Process Monitoring in Highly Sensitive Surgical Tasks. In Proceedings of the Nordic Sound and Music Computing Conference 2019 (Nordic SMC 2019). Stockholm, Sweden: KTH. Access the full text here . Sonification as a Reliable Alternative to Conventional Visual Surgical Navigation This study explores the application of sonification\u2014converting data into sound\u2014as an alternative to conventional visual navigation systems in surgical settings. The traditional visual systems, though accurate, often lack in usability and integration into surgical workflows. The paper introduces a novel four-degree-of-freedom (4-DOF) sonification method for navigated pedicle screw placement, utilizing frequency modulation synthesis to guide surgeons. A phantom study with 17 surgeons demonstrated that this method matches the accuracy of visual navigation while reducing the reliance on visual displays, allowing surgeons to focus more on the surgical tools and the anatomy involved. Citation: Matinfar, S., Salehi, M., Suter, D., Seibold, M., Dehghani, S., Navab, N., Wanivenhaus, F., F\u00fcrnstahl, P., Farshad, M., & Navab, N. (2023). Sonification as a reliable alternative to conventional visual surgical navigation. Scientific Reports, 13, Article 5930. https://doi.org/10.1038/s41598-023-32778-z Access the full text here . Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures This chapter, presented at MICCAI 2017, explores the potential of augmenting surgical environments with musical tracks that automatically adapt to the surgical procedure's context. Recognizing the cognitive overload from visual feedback systems in surgery, the authors propose using audio augmentation to deliver crucial information through an additional perceptual channel. This work specifically focuses on aural augmentation for ophthalmic procedures through automated modifications of musical pieces. The proposed system aims to provide ergonomic and pleasant auditory feedback without leading to fatigue, which is a common issue with simpler auditory feedback systems. The preliminary findings suggest that this sonification approach not only enhances the surgical environment but also does so in a way that is intuitive and aesthetically pleasing. The system alters musical attributes such as tempo and pitch in response to changes in surgical conditions, thereby helping maintain the surgeon's focus and reducing cognitive strain. Citation: Matinfar, S., Nasseri, M.A., Eck, U., Roodaki, H., Navab, N., Lohmann, C.P., Maier, M., & Navab, N. (2017). Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures. In: Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S. (eds) Medical Image Computing and Computer-Assisted Intervention \u2212 MICCAI 2017. Lecture Notes in Computer Science, vol 10434. Springer, Cham. https://doi.org/10.1007/978-3-319-66185-8_76 Access the full text here . Surgical Soundtracks: Automatic Acoustic Augmentation of Surgical Procedures This research paper focuses on the automatic acoustic augmentation of surgical procedures through the use of surgical soundtracks. The study explores how advances in sensing and digitalization enable the presentation of diverse datasets to enhance clinical decisions. Traditional visual feedback, while dominant, risks overstimulation and the loss of crucial information. The proposed solution involves using auditory augmentation to complement visual feedback, thereby enhancing the cognitive field without overwhelming it. The methods described involve the automatic modification of musical pieces to provide pleasant and ergonomic auditory feedback, aimed at avoiding alarm fatigue and simplifying complex routines. The research introduces a novel sonification method for the automatic musical augmentation of tasks within surgical procedures. The approach is designed to be aesthetically pleasing and efficient in conveying important information, potentially reducing cognitive load and increasing precision in the operating room. Citation: Matinfar, S., Nasseri, M.A., Eck, U., Kowalsky, M., Roodaki, H., Navab, N., Lohmann, C.P., Maier, M., Navab, N. (2018). Surgical soundtracks: automatic acoustic augmentation of surgical procedures. International Journal of Computer Assisted Radiology and Surgery, 13, 1345-1355. https://doi.org/10.1007/s11548-018-1827-2 Access the full text here . Three-dimensional Sonification as a Surgical Guidance Tool This research paper delves into the use of three-dimensional sonification as a navigation aid in surgical procedures, particularly in contexts where visual or haptic landmarks are minimal. The study conducted involved non-clinicians navigating a skull phantom, guided by auditory, visual, and audiovisual cues. The findings demonstrate that sonification, especially when combined with visual guidance, significantly enhances the precision of navigation in three-dimensional spaces without overwhelming the clinician. This suggests a potential reduction in cognitive load and an increase in procedural accuracy. Interactive sonification is known for its effectiveness in navigation tasks, particularly in medical scenarios like neuronavigation and image-guided surgery. This paper presents a case of simulated craniotomy preparation where sonification combined with visual cues helps achieve high precision in identifying targets on a skull phantom. Citation: Ziemer, T. (2023). Three-dimensional sonification as a surgical guidance tool. Journal on Multimodal User Interfaces. https://doi.org/10.1007/s12193-023-00422-9 Access the full text here .","title":"Related Work"},{"location":"related-work/#related-work","text":"","title":"Related Work"},{"location":"related-work/#sonification-for-process-monitoring-in-highly-sensitive-surgical-tasks","text":"This publication discusses innovative sonification methods for monitoring fluid-related processes in sensitive surgical environments. The techniques convert data on fluid rates and volumes into auditory signals, aiming to enhance situational awareness. The effectiveness of these methods was evaluated through user studies to determine the best auditory representations for surgical use. Citation: Matinfar, S., Hermann, T., Seibold, M., F\u00fcrnstahl, P., Farshad, M., & Navab, N. (In Press). Sonification for Process Monitoring in Highly Sensitive Surgical Tasks. In Proceedings of the Nordic Sound and Music Computing Conference 2019 (Nordic SMC 2019). Stockholm, Sweden: KTH. Access the full text here .","title":"Sonification for Process Monitoring in Highly Sensitive Surgical Tasks"},{"location":"related-work/#sonification-as-a-reliable-alternative-to-conventional-visual-surgical-navigation","text":"This study explores the application of sonification\u2014converting data into sound\u2014as an alternative to conventional visual navigation systems in surgical settings. The traditional visual systems, though accurate, often lack in usability and integration into surgical workflows. The paper introduces a novel four-degree-of-freedom (4-DOF) sonification method for navigated pedicle screw placement, utilizing frequency modulation synthesis to guide surgeons. A phantom study with 17 surgeons demonstrated that this method matches the accuracy of visual navigation while reducing the reliance on visual displays, allowing surgeons to focus more on the surgical tools and the anatomy involved. Citation: Matinfar, S., Salehi, M., Suter, D., Seibold, M., Dehghani, S., Navab, N., Wanivenhaus, F., F\u00fcrnstahl, P., Farshad, M., & Navab, N. (2023). Sonification as a reliable alternative to conventional visual surgical navigation. Scientific Reports, 13, Article 5930. https://doi.org/10.1038/s41598-023-32778-z Access the full text here .","title":"Sonification as a Reliable Alternative to Conventional Visual Surgical Navigation"},{"location":"related-work/#surgical-soundtracks-towards-automatic-musical-augmentation-of-surgical-procedures","text":"This chapter, presented at MICCAI 2017, explores the potential of augmenting surgical environments with musical tracks that automatically adapt to the surgical procedure's context. Recognizing the cognitive overload from visual feedback systems in surgery, the authors propose using audio augmentation to deliver crucial information through an additional perceptual channel. This work specifically focuses on aural augmentation for ophthalmic procedures through automated modifications of musical pieces. The proposed system aims to provide ergonomic and pleasant auditory feedback without leading to fatigue, which is a common issue with simpler auditory feedback systems. The preliminary findings suggest that this sonification approach not only enhances the surgical environment but also does so in a way that is intuitive and aesthetically pleasing. The system alters musical attributes such as tempo and pitch in response to changes in surgical conditions, thereby helping maintain the surgeon's focus and reducing cognitive strain. Citation: Matinfar, S., Nasseri, M.A., Eck, U., Roodaki, H., Navab, N., Lohmann, C.P., Maier, M., & Navab, N. (2017). Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures. In: Descoteaux, M., Maier-Hein, L., Franz, A., Jannin, P., Collins, D.L., Duchesne, S. (eds) Medical Image Computing and Computer-Assisted Intervention \u2212 MICCAI 2017. Lecture Notes in Computer Science, vol 10434. Springer, Cham. https://doi.org/10.1007/978-3-319-66185-8_76 Access the full text here .","title":"Surgical Soundtracks: Towards Automatic Musical Augmentation of Surgical Procedures"},{"location":"related-work/#surgical-soundtracks-automatic-acoustic-augmentation-of-surgical-procedures","text":"This research paper focuses on the automatic acoustic augmentation of surgical procedures through the use of surgical soundtracks. The study explores how advances in sensing and digitalization enable the presentation of diverse datasets to enhance clinical decisions. Traditional visual feedback, while dominant, risks overstimulation and the loss of crucial information. The proposed solution involves using auditory augmentation to complement visual feedback, thereby enhancing the cognitive field without overwhelming it. The methods described involve the automatic modification of musical pieces to provide pleasant and ergonomic auditory feedback, aimed at avoiding alarm fatigue and simplifying complex routines. The research introduces a novel sonification method for the automatic musical augmentation of tasks within surgical procedures. The approach is designed to be aesthetically pleasing and efficient in conveying important information, potentially reducing cognitive load and increasing precision in the operating room. Citation: Matinfar, S., Nasseri, M.A., Eck, U., Kowalsky, M., Roodaki, H., Navab, N., Lohmann, C.P., Maier, M., Navab, N. (2018). Surgical soundtracks: automatic acoustic augmentation of surgical procedures. International Journal of Computer Assisted Radiology and Surgery, 13, 1345-1355. https://doi.org/10.1007/s11548-018-1827-2 Access the full text here .","title":"Surgical Soundtracks: Automatic Acoustic Augmentation of Surgical Procedures"},{"location":"related-work/#three-dimensional-sonification-as-a-surgical-guidance-tool","text":"This research paper delves into the use of three-dimensional sonification as a navigation aid in surgical procedures, particularly in contexts where visual or haptic landmarks are minimal. The study conducted involved non-clinicians navigating a skull phantom, guided by auditory, visual, and audiovisual cues. The findings demonstrate that sonification, especially when combined with visual guidance, significantly enhances the precision of navigation in three-dimensional spaces without overwhelming the clinician. This suggests a potential reduction in cognitive load and an increase in procedural accuracy. Interactive sonification is known for its effectiveness in navigation tasks, particularly in medical scenarios like neuronavigation and image-guided surgery. This paper presents a case of simulated craniotomy preparation where sonification combined with visual cues helps achieve high precision in identifying targets on a skull phantom. Citation: Ziemer, T. (2023). Three-dimensional sonification as a surgical guidance tool. Journal on Multimodal User Interfaces. https://doi.org/10.1007/s12193-023-00422-9 Access the full text here .","title":"Three-dimensional Sonification as a Surgical Guidance Tool"}]}